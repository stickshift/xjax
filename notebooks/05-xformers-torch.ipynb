{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ba60b0-6a80-4669-a996-09cd1bfa49f7",
   "metadata": {},
   "source": [
    "# Demystifying Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67776602-a445-4967-9df1-eab7824d7258",
   "metadata": {},
   "source": [
    "Transformers are taking over machine learning. Since they were first described in 2017 (Vaswani et al, 2017), Transformers have come to define the entire NLP category, and are now spreading to image processing, reinforcement learning, and beyond. It's obvious that anyone who wants to contribute to the future of AI needs to learn the Transformer architecture inside and out. But this is easier said than done. While the Internet is full of papers, blog posts, and tutorials on Transformers, all of that material can be overwhelming.\n",
    "\n",
    "One key challenge I found when first studying Transformers was distilling the abstract ideas in the research literature into concrete, actionable steps I could experiment with. I wanted to \"see the code\" so to speak. While it's easy to find open source Transformer implementations, I found they are often overloaded with configuration knobs and dials, conditionalized this way and that to support every Transformer variation ever imagined. All of this reusability likely adds value for the Transformer experts who wrote them, but it makes it difficult to see the big picture anymore.\n",
    "\n",
    "My goal in this post is to help you get started with Transformers by walking through the core elements of the Transformer architecture step-by-step, connecting the abstract ideas from the literature to concrete lines of code without heaps of configuration logic piled on top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e429a-c8a2-412c-b06e-8186d1877928",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Unlike many machine learning models that process one input at a time, Transformers are *sequence models*. This means they process multiple inputs represented as an ordered list or *sequence*. This is a big deal. Not only can sequence models learn about the content of each input, they also learn about relationships between inputs. This makes sequence models ideally suited for a wide range of tasks that involve context such as processing text, video, and timeseries.\n",
    "\n",
    "Transformers are not the only sequence models out there. Transformers were introduced in 2017 as an improvement over earlier recurrent and convolutional approaches. These earlier approaches were already using \"attention\" mechanisms. The key innovation in the Transformer was realizing that the combination of attention and feedforward networks was powerful enough that the extra complexity of the RNN and CNN architectures was no longer needed. Not only did the Transformer perform better than previous approaches, the simpler neural network architecture proved to be faster and easier to train as well. (Vaswani et al, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8452270-2131-495a-a036-2b26416db57c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "316294b7-5364-49a7-9a06-c30491354233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewyoung/Development/stickshift/xjax/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.functional import relu, softmax\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae39acd-b1d7-4d16-b999-2d976722fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure gpu\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714fb86-c102-4ebb-8aeb-5967bf740742",
   "metadata": {},
   "source": [
    "# Transformer Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aee919-8316-4c71-bfb0-1e1462b79415",
   "metadata": {},
   "source": [
    "The following diagram depicts a Transformer as a multi-stage pipeline. The center of the Transformer contains a stack of transformer layers that are responsible for mapping input embeddings to output embeddings. This is where most of the magic happens. The stages before and after the Transformer Stack provide extra machinery required first to transform raw data into input embeddings and then output embeddings into task-specific outputs.\n",
    "\n",
    "<center><img src=\"img/transformer-pipeline.svg\" width=\"500\"></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587fa19-1a86-43f3-abde-16023ce00540",
   "metadata": {},
   "source": [
    "# Text Classifier\n",
    "\n",
    "We'll use a basic text classification pipeline to illustrate the process. The following cells use Hugging Face's transformers package to create an end-to-end transformer pipeline that classifies text as either positive or negative. There is a lot happening in very few lines of code. Over the rest of this post, we'll break the pipeline down and walk through each step from \"I love ice cream\" to the POSITIVE classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0e7063-6735-4725-ae95-664946753aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Create off-the-shelf text classification transformer\n",
    "transformer = transformers.pipeline(\"text-classification\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34aa67d7-bcc1-49ef-ae5e-84d41812e20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998118281364441}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(\"I love ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47ab5266-76b6-47e3-9213-ac684b386ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9974052309989929}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer(\"I hate ice cream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d01d53b-98e5-4551-b1cb-61fe51f82ed2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save config params for later\n",
    "config = transformer.model.distilbert.config\n",
    "vocab_size = config.vocab_size\n",
    "d_model = config.dim\n",
    "n_heads = config.n_heads\n",
    "d_head = int(d_model / n_heads)\n",
    "n_layers = config.n_layers\n",
    "max_sequence_length = config.max_position_embeddings\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ae0c9-b326-4645-bf5b-efb58e67b9b8",
   "metadata": {},
   "source": [
    "# Pre Process\n",
    "\n",
    "The Pre Process stage of a transformer is responsible for transforming raw input data into a sequence of categorical values. For NLP tasks, this is accomplished using a `Tokenizer` that parses raw text into tokens and maps the tokens to integer-encoded categorical values using a fixed vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0257b998-25ea-4184-9ddd-e41134cf8ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup tokenizer in transformer\n",
    "tokenizer = transformer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fb69317-03ae-4ddd-9657-4a7f088e01b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2293, 3256, 6949,  102]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='mps:0')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert \"I love ice cream\" into input values\n",
    "batch = tokenizer(\"I love ice cream\", return_tensors=\"pt\").to(device)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19827f46-23b4-456c-8489-9c4b0eb4f99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'i', 'love', 'ice', 'cream', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode input values so we can see what they represent \n",
    "[tokenizer.decode(input_id) for input_id in batch[\"input_ids\"][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0fc6f4-d081-4a46-a822-660209d198c3",
   "metadata": {},
   "source": [
    "Here we can see the tokenizer parsed \"I love ice cream\" into 6 input values 101, 1045, 2293, 3256, 6949, 102. There is one value for each word as well [CLS] and [SEP] markers to flag the beginning and end of the original sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631afd9f-ce16-4ce3-a144-76c8511885a9",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "The Embeddings stage of a transformer transforms the input sequence into embeddings. This is typically done using two lookup tables. The first table maps the input values to embeddings. The second table maps the input positions to embeddings. The value and position embeddings are then added together to create position-encoded input embeddings.\n",
    "\n",
    "<center><img src=\"img/embeddings.svg\" width=\"700\"></center> \n",
    "\n",
    "Both the value and position embeddings are initialized randomly and then learned during training. This means the embeddings are transformer-specific. You can't take embeddings from one transformer and use them in another without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f17d34c8-6a18-4d8b-af40-856f88c6ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup embeddings in transformer\n",
    "value_embeddings = transformer.model.distilbert.embeddings.word_embeddings\n",
    "position_embeddings = transformer.model.distilbert.embeddings.position_embeddings\n",
    "normalize = transformer.model.distilbert.embeddings.LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9573fc3c-0a64-4d4c-99e5-ae2fe6e9e624",
   "metadata": {},
   "source": [
    "The input value embedding layer maps the integer-encoded categorical input values to unique embedding vectors. In the following cell, you can see `value_embeddings` has 30,522 768-element embedding vectors. 30,522 is the transformer's vocabulary size (`vocab_size`). 768 is the transformer's model dimensions (`d_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b692e0-b648-4bd5-869b-511d1656c43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48249288-16da-42d2-92c2-be5b48262680",
   "metadata": {},
   "source": [
    "The input position embedding layer maps the positions of each input value to a set of embeddings. In the following cell, you can see `position_embeddings` has 512 768-element embedding vectors. 512 is the maximum sequence length the transformer can process (`max_sequence_length`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c48624d-0aa1-4ca7-9cc9-9231d5619bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(512, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a049a-b37b-4d16-bddd-1eeaa211407e",
   "metadata": {},
   "source": [
    "Next, let's take convert the input_ids from the Pre Process stage into position-encoded input embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a07bf0-b044-4a11-a8c8-ef790409bf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1045, 2293, 3256, 6949,  102]], device='mps:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = batch[\"input_ids\"]\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "daeade7e-324d-450e-b597-7f785c059a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup value embeddings for each input\n",
    "v = value_embeddings(input_ids)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9acd5b56-218d-44a0-b355-eadb3d6b8251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.9925e-02, -1.0171e-02, -2.0390e-02,  ...,  6.1588e-02,\n",
       "           2.1959e-02,  2.2732e-02],\n",
       "         [-1.2794e-02,  4.9879e-03, -2.6270e-02,  ..., -7.2300e-05,\n",
       "           5.3657e-03,  1.1908e-02],\n",
       "         [ 5.9359e-02, -2.3563e-02, -2.0560e-03,  ..., -1.0420e-02,\n",
       "           1.4846e-02, -1.2815e-02],\n",
       "         [-2.4101e-02, -2.4911e-02, -2.2601e-02,  ..., -2.5139e-02,\n",
       "           1.1392e-02,  3.2655e-02],\n",
       "         [-8.5466e-02, -5.9276e-02, -5.6659e-02,  ..., -1.7192e-02,\n",
       "          -8.6179e-02, -4.5105e-02],\n",
       "         [-2.1060e-02, -6.4941e-03, -1.0682e-02,  ..., -2.3401e-02,\n",
       "           6.1463e-03, -6.4845e-03]]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52f03650-29d9-4666-87cd-9071b9bc613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5]], device='mps:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(input_ids.size(1)).expand(1, -1).to(device)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18533121-5a9f-421f-a2d0-bf9936ac90ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lookup position embeddings for each input\n",
    "p = position_embeddings(position_ids)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5876647-e5e6-4440-96a3-9e36e3fde0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8007e-02, -2.3798e-02, -3.5982e-02,  ...,  4.5726e-04,\n",
       "           5.1363e-05,  1.5002e-02],\n",
       "         [ 7.8592e-03,  4.8144e-03, -1.6093e-02,  ...,  2.9312e-02,\n",
       "           2.7634e-02, -8.5431e-03],\n",
       "         [-1.1663e-02, -3.1590e-03, -9.4000e-03,  ...,  1.4870e-02,\n",
       "           2.1609e-02, -7.4069e-03],\n",
       "         [-4.0848e-03, -1.1123e-02, -2.1704e-02,  ...,  1.8962e-02,\n",
       "           4.6763e-03, -1.0220e-03],\n",
       "         [-8.2666e-03, -4.1641e-03, -7.5136e-03,  ...,  1.9757e-02,\n",
       "          -2.2192e-03,  3.8681e-03],\n",
       "         [ 4.6293e-04, -1.8499e-02, -1.9709e-02,  ...,  5.4042e-03,\n",
       "           1.8076e-02,  2.9490e-03]]], device='mps:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbd2fbd7-f525-4c58-8447-5c06291aa605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine value and position embeddings and normalize\n",
    "input_embeddings = normalize(v + p)\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8face629-9b4b-48e7-b796-c89829268d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3549, -0.1386, -0.2253,  ...,  0.1536,  0.0748,  0.1310],\n",
       "         [ 0.2282,  0.5511, -0.5092,  ...,  0.6421,  0.9541,  0.3192],\n",
       "         [ 1.4511, -0.0794,  0.2168,  ...,  0.2851,  1.0723, -0.0919],\n",
       "         [-0.0564, -0.1761, -0.2870,  ...,  0.1442,  0.6767,  1.0396],\n",
       "         [-1.1349, -0.5135, -0.4714,  ...,  0.3874, -1.0348, -0.2812],\n",
       "         [-0.2980, -0.3332, -0.3742,  ..., -0.3392,  0.3764, -0.1298]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd78279-8602-4369-8116-343731a0b7f0",
   "metadata": {},
   "source": [
    "At this point, `input_embeddings` contains a position-encoded input embedding vector for each value in the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae0098-0a0b-4c4f-bb6b-780d54565523",
   "metadata": {},
   "source": [
    "# Transformer Stack\n",
    "\n",
    "The Transformer Stack is where the \"magic\" happens. The stack is responsible for transforming position-encoded input embeddings into contextualized output embeddings through layers of attention and feed-forward networks. The difference between most transformer architectures such as BERT, BART, GPT is in the number of layers, the size of each layer, and the type of attention used.\n",
    "\n",
    "<center><img src=\"img/transformer-layer.svg\" width=\"500\"></center>\n",
    "\n",
    "Let's take a look at one of the layers in our text classification transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9d15b404-d6ac-49f2-8f22-dfc009e0d742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (attention): MultiHeadSelfAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (ffn): FFN(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "    (activation): GELUActivation()\n",
       "  )\n",
       "  (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = transformer.model.distilbert.transformer.layer[0]\n",
    "\n",
    "# Extract building blocks from layer\n",
    "query_projection = layer.attention.q_lin\n",
    "key_projection = layer.attention.k_lin\n",
    "value_projection = layer.attention.v_lin\n",
    "output_projection = layer.attention.out_lin\n",
    "normalize_attention = layer.sa_layer_norm\n",
    "ffn = layer.ffn\n",
    "normalize_ffn = layer.output_layer_norm\n",
    "\n",
    "layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32c1459-3dd7-43eb-b18a-0eee85b1c843",
   "metadata": {},
   "source": [
    "## Multi-Head Self Attention\n",
    "\n",
    "You can see in the cell above that transformer is using Multi-Head Self Attention. This is the standard attention algorithm described in the original Attention is All You Need paper (Vaswani et al, 2017). While many transformers still use the original algorithm, others are experimenting with new forms of attention (Dubey et al. 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac77b63f-179d-4bb8-8872-154630e983df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x):\n",
    "    return x.view(batch_size, -1, n_heads, d_head).transpose(1, 2)\n",
    "\n",
    "def combine_heads(x):\n",
    "    return x.transpose(1, 2).contiguous().view(batch_size, -1, int(n_heads * d_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efe41640-5bfb-42aa-8e22-16cc71b7d1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 768]), torch.Size([1, 6, 768]), torch.Size([1, 6, 768]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project input_embeddings into \"query space\"\n",
    "q = query_projection(input_embeddings)\n",
    "\n",
    "# Project input_embeddings into \"key space\"\n",
    "k = key_projection(input_embeddings)\n",
    "\n",
    "# Project input_embeddings into \"value space\"\n",
    "v = value_projection(input_embeddings)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c79830b-5ff5-490e-b578-71dac3a725ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 6, 64]),\n",
       " torch.Size([1, 12, 6, 64]),\n",
       " torch.Size([1, 12, 6, 64]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split q, k, v into multiple heads\n",
    "q = split_heads(q)\n",
    "k = split_heads(k)\n",
    "v = split_heads(v)\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4451a42c-d38d-4d51-9672-4a62da19c539",
   "metadata": {},
   "source": [
    "Next, we calculate attention using standard formula: $Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebdca3b1-71db-46b3-a625-b8d0c6173e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 6, 64])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate attention for all heads in parallel\n",
    "attention = softmax(q @ k.transpose(2, 3) / sqrt(d_head), dim=-1) @ v\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14498b14-22a7-483e-88c3-b6df4aa4c2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine heads back together\n",
    "attention = combine_heads(attention)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f363baa-ae50-4828-b442-8a2748ce6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project attention back into \"model space\"\n",
    "attention_embeddings = output_projection(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9136231-4c49-49eb-8e58-b88a5616a2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine attention and input embeddings\n",
    "attention_embeddings = normalize_attention(attention_embeddings + input_embeddings)\n",
    "attention_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f678cf54-9cae-4963-b5fb-be6719cc71d9",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c84d78df-a1c3-49d5-bee1-99fe8df89770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform attention embeddings\n",
    "ffn_embeddings = ffn(attention_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80f253dc-c703-42e0-93a9-1bdb6dc53591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine ffn and attention embeddings\n",
    "ffn_embeddings = normalize_ffn(ffn_embeddings + attention_embeddings)\n",
    "ffn_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f887d8a-f3af-4667-801b-469b041ccbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2824, -0.0438, -0.1085,  ...,  0.0478, -0.0888, -0.1025],\n",
       "         [ 0.5897,  0.7285,  0.0869,  ...,  0.1732,  0.5214,  0.4234],\n",
       "         [ 1.7403,  0.1464,  0.4697,  ...,  0.2500,  0.8521, -0.2792],\n",
       "         [-0.1238, -0.3677,  0.1768,  ...,  0.8630,  0.8072,  0.3339],\n",
       "         [-0.3699, -0.2425, -0.5013,  ...,  0.7443, -0.5795, -0.6643],\n",
       "         [-0.0757, -0.0300, -0.0669,  ..., -0.1338,  0.1781,  0.0099]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ffn_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b06996-e8be-4616-a15a-727381d25753",
   "metadata": {},
   "source": [
    "## N-Layers\n",
    "\n",
    "Now that we've walked through a single layer step-by-step, let's put the pieces together and apply the entire stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef03eab4-e9dc-485d-bd22-3ba9847f3fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = input_embeddings\n",
    "\n",
    "for i in range(n_layers):\n",
    "    layer = transformer.model.distilbert.transformer.layer[i]\n",
    "\n",
    "    # Extract building blocks from layer\n",
    "    query_projection = layer.attention.q_lin\n",
    "    key_projection = layer.attention.k_lin\n",
    "    value_projection = layer.attention.v_lin\n",
    "    output_projection = layer.attention.out_lin\n",
    "    normalize_attention = layer.sa_layer_norm\n",
    "    ffn = layer.ffn\n",
    "    normalize_ffn = layer.output_layer_norm\n",
    "\n",
    "    # Project embeddings into \"query space\"\n",
    "    q = query_projection(embeddings)\n",
    "    \n",
    "    # Project embeddings into \"key space\"\n",
    "    k = key_projection(embeddings)\n",
    "    \n",
    "    # Project embeddings into \"value space\"\n",
    "    v = value_projection(embeddings)\n",
    "\n",
    "    # Split q, k, v into multiple heads\n",
    "    q = split_heads(q)\n",
    "    k = split_heads(k)\n",
    "    v = split_heads(v)\n",
    "\n",
    "    # Calculate attention for all heads in parallel\n",
    "    attention = softmax(q @ k.transpose(2, 3) / sqrt(d_head), dim=-1) @ v\n",
    "\n",
    "    # Combine heads back together\n",
    "    attention = combine_heads(attention)\n",
    "    \n",
    "    # Project attention back into \"model space\"\n",
    "    attention_embeddings = output_projection(attention)\n",
    "    \n",
    "    # Combine attention and embeddings\n",
    "    attention_embeddings = normalize_attention(attention_embeddings + embeddings)\n",
    "    \n",
    "    # Transform attention embeddings\n",
    "    ffn_embeddings = ffn(attention_embeddings)\n",
    "    \n",
    "    # Combine ffn and attention embeddings\n",
    "    ffn_embeddings = normalize_ffn(ffn_embeddings + attention_embeddings)\n",
    "\n",
    "    # Rinse and repeat\n",
    "    embeddings = ffn_embeddings\n",
    "    \n",
    "output_embeddings = embeddings\n",
    "output_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6394c0e7-4c04-4a3c-bcf6-de9c542793e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.6173e-01, -1.3168e-01,  3.5342e-02,  ...,  4.4015e-01,\n",
       "           1.0666e+00, -1.9293e-01],\n",
       "         [ 7.3341e-01,  4.9823e-02, -1.7590e-02,  ...,  5.0063e-01,\n",
       "           1.1480e+00, -1.2997e-01],\n",
       "         [ 1.1230e+00,  2.7603e-01,  3.2096e-01,  ...,  1.8820e-01,\n",
       "           1.0586e+00, -1.2496e-01],\n",
       "         [ 4.8728e-01,  1.4863e-02,  4.2930e-01,  ...,  4.8993e-01,\n",
       "           7.9436e-01,  1.2331e-01],\n",
       "         [ 1.0596e-03, -1.4508e-01,  2.8892e-01,  ...,  5.5342e-01,\n",
       "           7.9370e-01, -9.0898e-02],\n",
       "         [ 1.1021e+00,  8.6115e-02,  5.7461e-01,  ...,  6.8800e-01,\n",
       "           5.6345e-01, -6.6278e-01]]], device='mps:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0194ed-0941-4dd8-9f68-a67fe6fb7f52",
   "metadata": {},
   "source": [
    "# Head\n",
    "\n",
    "The Head stage of a transformer takes the contextualized embeddings and applies a task-specific transformation to reach the desired output. In our case, we're running a binary text classifier so the goal of the Head stage is to convert contextualized embeddings into a binary prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3758fe4-0f22-4d7f-b8af-9689022267ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup head classifier in transformer\n",
    "pre_classifier = transformer.model.pre_classifier\n",
    "classifier = transformer.model.classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332cc1d-8982-4753-ac6d-8b2d99a83ded",
   "metadata": {},
   "source": [
    "The head classifier is a standard FNN. But what do we pass into it? The FNN expects one set of features to predict on and we have a sequence of feature vectors.\n",
    "\n",
    "Since the output embeddings are \"contextualized\", it's common practice to use the first embedding to represent the entire sequence and drop the rest. Note that you couldn't do that with the input embeddings because they represented each input value independently. It's only after the transformer has added context from the rest of the sequence that the first embedding can meaningfully represent the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4df13e8a-7ba3-44bd-9d17-f4ccebd4736b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Represent sequence with contextualized embedding for start marker [CLS]\n",
    "embedding = output_embeddings[:, 0]\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "405366ee-dea3-441a-9ff2-eb493e39c5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform embedding\n",
    "embedding = relu(pre_classifier(embedding))\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7a59124-a082-40a2-bbeb-5cbce6750533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classify embedding\n",
    "logits = classifier(embedding)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d5e989e-01c7-43c0-bc63-c219537fc673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.1625,  4.4154]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d41315f-b0fb-49fa-9e4c-de7155dd98de",
   "metadata": {},
   "source": [
    "# Post Process\n",
    "\n",
    "The final stage in the Transformer is to convert the predicted logits into probabilities. In the cells below, you can see we get the same prediction that we started with when running the high level pipeline API. This should give you confidence that the steps we walked through covered everything from raw text to positive predicition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9894f4f-e189-4943-b171-2cb185807c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8818e-04, 9.9981e-01], device='mps:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert logits into probabilities\n",
    "scores = softmax(logits[0])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aaa2863f-7a93-4b6e-8e17-e4c3f418e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move scores back to cpu\n",
    "scores = scores.detach().to(\"cpu\").float().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24199c8f-3e06-4ef0-bec5-a974361e02d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POSITIVE': np.float32(0.9998118)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map scores to labels\n",
    "{config.id2label[scores.argmax()]: scores[scores.argmax()]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21912584-062a-4599-9254-f3799c1a2fcd",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660e863-8e0f-4b47-a62e-f95fcc3b4de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197504d3-2e2d-4425-b6f2-bc8194228a3c",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb90fcdf-7122-41c9-90b4-917281abda5f",
   "metadata": {},
   "source": [
    "Dubey, Abhimanyu, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, et al. “The Llama 3 Herd of Models.” arXiv, July 31, 2024. https://doi.org/10.48550/arXiv.2407.21783.\n",
    "\n",
    "Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” arXiv, 2017. https://doi.org/10.48550/arXiv.1706.03762."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c9662-71af-44b8-9e28-2d1afca2e771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
